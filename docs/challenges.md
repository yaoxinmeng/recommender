# Challenges
1. Built-in APIs from LangChain does not work great out of the box
    - LangChain's Playwright wrapper makes multiple `asyncio.run` calls, and I am unsure if it will interfere with FastAPI's event loop. It was much simpler to just directly invoke `sync_playwright()` from the Playwright package.
    - LangChain's wrapper for BeautifulSoup does not offer the fine-grained control over the contents and formatting of the scraped text. This is understandable, since it is meant to be a high level package, but it was not adequate for this particular use case where images embedded in the document had to be extracted. Instead, I wrote a custom BeautifulSoup parser for this application. 
    - LangChain's wrapper for Bedrock does not work with Amazon Nova. I did not manage to find out if it was an issue faced by all multimodal LLMs on Bedrock, or is this just specific to Nova since it was recently released. Due to this issue, I had to write my own wrapper to call boto3 for the multimodal portion of the task.
    - LangChain's wrapper for structured prompts does not work with their BedrockLLM class, and even though it works with their ChatBedrockConverse class, it yielded poor results. In my attempts to reproduce the prompting template for structured outputs I also found that using Pydantic models as is tend to yield poor results. The best results I obtained were from manually writing [structured JSON objects](/backend/app/types/model_json_schema.py) for each Pydantic model that I want as output. However, this could just be an edge case for the older Mistral models. 
2. Maximum allowed LLM invocation rate is very low for personal AWS account (consecutive calls to LLM will raise `ThrottlingException` that must be manually handled). 
    - This makes development work slower than expected, since each run of the pipeline takes a few minutes to process (multiple retries using the customised backoff function).
    - This also limits the types of workflow I can realistically design, since any agentic system that involves too many iterative LLM calls will take so long to run that it will be impossible to test. 
    - To work around this limitation locally but also ensure that the system is still robust in production environment, I have made the number of iterations configurable via API. This can be adjusted based on the desired accuracy of the results. 
3. Large models with better performance are prohibitively expensive and not feasible for experimentation with an agentic system that will be making iterative LLM calls. As such, the system could only be tested with smaller models, and performance had to be boosted through other means such as prompt engineering. In the future, this system could be upgraded to a more powerful model with some slight tweaking of prompt templates.
4. The LLMs that I have tested (Mistral 7B, Mixtral 8x7B, Llama 3 70B) tend to be inconsistent in their answer format, even with explicit system prompts. This is despite the low temperature value, so the only workaround I had for this was to build custom parsers using regex to cover edge cases. While this problem can also be circumvented by making iterative calls with the same prompt and selecting the best answer, it was not feasible in my case due to the challenges outlined above.
5. I attempted to use LangGraph for the agentic portion of the workflow (mainly [Stage 2](/README.md#2-retrieving-relevant-details-of-each-candidate-location)), but it did not work with most Bedrock LLMs. From the documentation, it seems that only Anthropic models were supported, but those were too expensive and also required use case justification for access. Therefore, I made the decision to build a simple custom single-agent pipeline instead.
6. Scraping information from websites is challenging and frequently yielded poor results
    - Playwright is launched in headless mode, which could be detected as a bot by many websites. The obvious solution is to run it in headed mode, but that requires XServer and is hardware-dependent, which is anti-pattern since Docker is meant to be hardware-agnostic. Therefore, I stuck to running it in headless mode for now.
    - Many websites render their content using JS scripts, which takes time to load. How long that takes is variable for each website, so I have configured Playwright to wait till no network events are fired, up to a maximum limit of 3s.