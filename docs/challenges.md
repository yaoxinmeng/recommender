# Challenges
1. Built-in APIs from LangChain does not work great out of the box
    - LangChain's Playwright wrapper makes multiple `asyncio.run` calls, and I am unsure if it will interfere with FastAPI's event loop. It was much simpler to just directly invoke `sync_playwright()` from the Playwright package.
    - LangChain's wrapper for BeautifulSoup does not offer the fine-grained control over the contents and formatting of the scraped text. This is understandable, since it is meant to be a high level package, but it was not adequate for this particular use case where images embedded in the document had to be extracted. Instead, I wrote a custom BeautifulSoup parser for this application. 
    - LangChain's wrapper for Bedrock does not work with Amazon Nova. I did not manage to find out if it was an issue faced by all multimodal LLMs on Bedrock, or is this just specific to Nova since it was recently released. Due to this issue, I had to write my own wrapper to call boto3 for the multimodal portion of the task.
2. Maximum allowed LLM invocation rate is very low for personal AWS account. 
    - This makes development work slower than expected, since each run of the pipeline takes a few minutes to process (multiple retries using the customised backoff function).
    - This also limits the types of workflow I can realistically design, since any agentic system that involves too many iterative LLM calls will take so long to run that it will be impossible to test. 
    - To work around this limitation locally but also ensure that the system is still robust in production environment, I have made the number of iterations configurable via API. This can be adjusted based on the desired accuracy of the results. 
3. Large models with better performance are prohibitively expensive and not feasible for experimentation with an agentic system that will be making iterative LLM calls. As such, the system could only be tested with smaller models, and performance had to be boosted through other means such as prompt engineering. In the future, this system could be upgraded to a more powerful model with some slight tweaking of prompt templates.
4. The LLMs that I have tested (Mistral 7B, Mixtral 8x7B, Llama 3 70B) tend to be inconsistent in their answer format, even with explicit system prompts. This is despite the low temperature value, so the only workaround I had for this was to build custom parsers using regex to cover edge cases. While this problem can also be circumvented by making iterative calls with the same prompt and selecting the best answer, it was not feasible in my case due to the challenges outlined above. 
5. I attempted to use LangGraph for the agentic portion of the workflow (mainly [Stage 2](/README.md#2-retrieving-relevant-details-of-each-candidate-location)), but it did not work with most Bedrock LLMs. From the documentation, it seems that only Anthropic models were supported, but those were too expensive and also required use case justification for access. Therefore, I made the decision to build a simple custom single-agent pipeline instead.